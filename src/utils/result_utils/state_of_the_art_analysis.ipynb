{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_txt(input_file,output_file):\n",
    "   \n",
    "    with open(input_file, 'r') as csv_file, open(output_file, 'w') as space_delimited_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            space_delimited_file.write(' '.join(row) + '\\n')\n",
    "\n",
    "    print(f'CSV file \"{input_file}\" converted to space-delimited file \"{output_file}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    hf = h5py.File(file, 'r')\n",
    "    attributes = []\n",
    "    for key in hf.keys():\n",
    "        attributes.append(key)\n",
    "    \n",
    "    return attributes, hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(hf,attributes):\n",
    "    data = []\n",
    "    pm = []\n",
    "    acc_pm = []\n",
    "    loss_pm = []\n",
    "    loss_gm = []\n",
    "    for i in range(len(attributes)):\n",
    "        ai = hf.get(attributes[i])\n",
    "        ai = np.array(ai)\n",
    "        data.append(ai)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(description, mean_std, file):\n",
    "    file.write(f\"{description} (mean/std) : ({mean_std[0]} / {mean_std[1]})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_result(path,directory_name, algorithm, avg_file, target):\n",
    "    \n",
    "    dir_list = os.listdir(path)\n",
    "\n",
    "    i=0\n",
    "    train_loss, train_accuracy, test_loss, test_accuracy = [], [], [], []\n",
    "    per_train_loss, per_train_accuracy, per_test_loss, per_test_accuracy = [], [], [], []\n",
    "    per_precision, per_recall, per_f1 = [], [], []\n",
    "    precision, recall, f1 = [], [], []\n",
    "\n",
    "    if algorithm in [ \"Local\", \"Fedavg\", \"Fesem\", \"Fedprox\"]:\n",
    "        for file_name in dir_list:\n",
    "            \n",
    "            \n",
    "            if file_name.endswith(\".h5\"):\n",
    "                print(file_name)\n",
    "                attributes, hf = read_file(path+file_name)\n",
    "\n",
    "                data = get_data(hf,attributes)\n",
    "                id=0\n",
    "                for key in hf.keys():\n",
    "                    attributes.append(key)\n",
    "                    # print(\"id [\",id,\"] :\", key)\n",
    "                    id+=1\n",
    "\n",
    "                gtsl = hf.get('global_test_loss')\n",
    "                gtrl = hf.get('global_train_loss')\n",
    "                gtsa = hf.get('global_test_accuracy')\n",
    "                gtra = hf.get('global_train_accuracy')\n",
    "                gp = hf.get('global_precision')\n",
    "                gr = hf.get('global_recall')\n",
    "                gf1 = hf.get('global_f1score')\n",
    "\n",
    "                test_loss.append(np.array(gtsl).tolist())\n",
    "                train_loss.append(np.array(gtrl).tolist())\n",
    "                test_accuracy.append(np.array(gtsa).tolist())\n",
    "                train_accuracy.append(np.array(gtra).tolist())\n",
    "                precision.append(np.array(gp).tolist())\n",
    "                recall.append(np.array(gr).tolist())\n",
    "                f1.append(np.array(gf1).tolist())\n",
    "\n",
    "\n",
    "                \n",
    "            \n",
    "        avg_train_loss = np.array(train_loss)\n",
    "        avg_test_loss = np.array(test_loss)\n",
    "        avg_train_accuracy = np.array(train_accuracy)\n",
    "        avg_test_accuracy = np.array(test_accuracy)\n",
    "        avg_precision = np.array(precision)\n",
    "        avg_recall = np.array(recall)\n",
    "        avg_f1 = np.array(f1)\n",
    "\n",
    "\n",
    "        # print(avg_test_accuracy)\n",
    "        \n",
    "        gtrl_mean = np.mean(avg_train_loss, axis=0)\n",
    "        gtra_mean = np.mean(avg_train_accuracy, axis=0)\n",
    "        gtsl_mean = np.mean(avg_test_loss, axis=0)\n",
    "        gtsa_mean = np.mean(avg_test_accuracy, axis=0)\n",
    "\n",
    "        gtrl_std = np.std(avg_train_loss, axis=0)\n",
    "        gtra_std = np.std(avg_train_accuracy, axis=0)\n",
    "        gtsl_std = np.std(avg_test_loss, axis=0)\n",
    "        gtsa_std = np.std(avg_test_accuracy, axis=0)\n",
    "\n",
    "        gp_mean = np.mean(avg_precision, axis=0)\n",
    "        gr_mean = np.mean(avg_recall, axis=0)\n",
    "        gf1_mean = np.mean(avg_f1, axis=0)\n",
    "\n",
    "        gp_std = np.std(avg_precision, axis=0)\n",
    "        gr_std = np.std(avg_recall, axis=0)\n",
    "        gf1_std = np.std(avg_f1, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        gtrl_mean_std = np.column_stack((gtrl_mean, gtrl_std))\n",
    "        gtra_mean_std = np.column_stack((gtra_mean, gtra_std))\n",
    "        gtsl_mean_std = np.column_stack((gtsl_mean, gtsl_std))\n",
    "        gtsa_mean_std = np.column_stack((gtsa_mean, gtsa_std))\n",
    "\n",
    "        gp_mean_std = np.column_stack((gp_mean, gp_std))\n",
    "        gr_mean_std = np.column_stack((gr_mean, gr_std))\n",
    "        gf1_mean_std = np.column_stack((gf1_mean, gf1_std))\n",
    "\n",
    "\n",
    "        training_loss_mean_std = gtrl_mean_std[gtrl_mean_std[:,0].argmin()]\n",
    "        training_acc_mean_std = gtra_mean_std[gtra_mean_std[:,0].argmax()]\n",
    "        val_loss_mean_std = gtsl_mean_std[gtsl_mean_std[:,0].argmin()]\n",
    "        val_acc_mean_std = gtsa_mean_std[gtsa_mean_std[:,0].argmax()]\n",
    "        precision_mean_std = gp_mean_std[gp_mean_std[:,0].argmax()]\n",
    "        recall_mean_std = gr_mean_std[gr_mean_std[:,0].argmax()]\n",
    "        f1_mean_std = gf1_mean_std[gf1_mean_std[:,0].argmax()]\n",
    "\n",
    "\n",
    "        \n",
    "        with h5py.File(directory_name  + '{}.h5'.format(avg_file), 'w') as hf:\n",
    "            hf.create_dataset('avg_training_loss', data=gtrl_mean)\n",
    "            hf.create_dataset('avg_training_accuracy', data=gtra_mean)\n",
    "            hf.create_dataset('avg_test_loss', data=gtsl_mean)\n",
    "            hf.create_dataset('avg_test_accuracy', data=gtsa_mean)\n",
    "            hf.create_dataset('avg_precision', data=gp_mean)\n",
    "            hf.create_dataset('avg_recall', data=gr_mean)\n",
    "            hf.create_dataset('avg_f1', data=gf1_mean)\n",
    "\n",
    "            hf.close\n",
    "\n",
    "\n",
    "\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\")\n",
    "        print(\"Algorithm :\",algorithm)\n",
    "        print(\"Global training loss (mean/std) : (\",training_loss_mean_std[0],\"/\",training_loss_mean_std[1],\")\")\n",
    "        print(\"Global training accuracy (mean/std) : (\",training_acc_mean_std[0],\"/\",training_acc_mean_std[1],\")\")\n",
    "        print(\"Global test loss (mean/std) : (\", val_loss_mean_std[0],\"/\", val_loss_mean_std[1],\")\")\n",
    "        print(\"Global test accuracy (mean/std) : (\",val_acc_mean_std[0],\"/\",val_acc_mean_std[1],\")\")\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")  \n",
    "        print(f\"Global Precision (mean/std) : ({precision_mean_std[0]} / {precision_mean_std[1]})\")\n",
    "        print(f\"Global Recall (mean/std) : ({recall_mean_std[0]} / {recall_mean_std[1]})\")\n",
    "        print(f\"Global F1Score (mean/std) : ({f1_mean_std[0]} / {f1_mean_std[1]})\")\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")  \n",
    "\n",
    "\n",
    "        file_name = 'performance_' + algorithm + '_' + str(target) + '.txt'\n",
    "        \n",
    "    \n",
    "        with open(directory_name + '/' + file_name, 'w') as file:\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")\n",
    "            file.write(f\"algorithm : {algorithm} : target {target}\")\n",
    "            print_stats(\"Global training loss\", training_loss_mean_std, file)\n",
    "            print_stats(\"Global training accuracy\", training_acc_mean_std, file)\n",
    "            print_stats(\"Global test loss\", val_loss_mean_std, file)\n",
    "            print_stats(\"Global test accuracy\", val_acc_mean_std, file)\n",
    "            \n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\\n\")\n",
    "            print_stats(\"Global Precision\", precision_mean_std, file)\n",
    "            print_stats(\"Global Recall\", recall_mean_std, file)\n",
    "            print_stats(\"Global F1Score\", f1_mean_std, file)\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\\n\")\n",
    "       \n",
    "    \n",
    "    if algorithm in ['pFedme', 'Fedmem', 'demlearn', 'h-sgd']:\n",
    "        for file_name in dir_list:\n",
    "            if file_name.endswith(\".h5\"):\n",
    "                print(file_name)\n",
    "                attributes, hf = read_file(path+file_name)\n",
    "\n",
    "                data = get_data(hf,attributes)\n",
    "                id=0\n",
    "                for key in hf.keys():\n",
    "                    attributes.append(key)\n",
    "                    # print(\"id [\",id,\"] :\", key)\n",
    "                    id+=1\n",
    "\n",
    "                gtsl = hf.get('global_test_loss')\n",
    "                gtrl = hf.get('global_train_loss')\n",
    "                gtsa = hf.get('global_test_accuracy')\n",
    "                gtra = hf.get('global_train_accuracy')\n",
    "\n",
    "                ptsl = hf.get('per_test_loss')\n",
    "                ptrl = hf.get('per_train_loss')\n",
    "                ptsa = hf.get('per_test_accuracy')\n",
    "                ptra = hf.get('per_train_accuracy')\n",
    "\n",
    "                gp = hf.get('global_precision')\n",
    "                gr = hf.get('global_recall')\n",
    "                gf1 = hf.get('global_f1score')\n",
    "\n",
    "                pp = hf.get('per_precision')\n",
    "                pr = hf.get('per_recall')\n",
    "                pf1 = hf.get('per_f1score')\n",
    "            \n",
    "                test_loss.append(np.array(gtsl).tolist())\n",
    "                train_loss.append(np.array(gtrl).tolist())\n",
    "                test_accuracy.append(np.array(gtsa).tolist())\n",
    "                train_accuracy.append(np.array(gtra).tolist())\n",
    "\n",
    "                per_test_loss.append(np.array(ptsl).tolist())\n",
    "                per_train_loss.append(np.array(ptrl).tolist())\n",
    "                per_test_accuracy.append(np.array(ptsa).tolist())\n",
    "                per_train_accuracy.append(np.array(ptra).tolist())\n",
    "\n",
    "                precision.append(np.array(gp).tolist())\n",
    "                recall.append(np.array(gr).tolist())\n",
    "                f1.append(np.array(gf1).tolist())\n",
    "\n",
    "                per_precision.append(np.array(pp).tolist())\n",
    "                per_recall.append(np.array(pr).tolist())\n",
    "                per_f1.append(np.array(pf1).tolist())\n",
    "\n",
    "                \n",
    "        avg_train_loss = np.array(train_loss)\n",
    "        avg_test_loss = np.array(test_loss)\n",
    "        avg_train_accuracy = np.array(train_accuracy)\n",
    "        avg_test_accuracy = np.array(test_accuracy)\n",
    "\n",
    "\n",
    "        avg_per_train_loss = np.array(per_train_loss)\n",
    "        avg_per_test_loss = np.array(per_test_loss)\n",
    "        avg_per_train_accuracy = np.array(per_train_accuracy)\n",
    "        avg_per_test_accuracy = np.array(per_test_accuracy)\n",
    "\n",
    "        avg_precision = np.array(precision)\n",
    "        avg_recall = np.array(recall)\n",
    "        avg_f1 = np.array(f1)\n",
    "\n",
    "        avg_per_precision = np.array(per_precision)\n",
    "        avg_per_recall = np.array(per_recall)\n",
    "        avg_per_f1 = np.array(per_f1)\n",
    "\n",
    "\n",
    "        gtrl_mean = np.mean(avg_train_loss, axis=0)\n",
    "        gtra_mean = np.mean(avg_train_accuracy, axis=0)\n",
    "        gtsl_mean = np.mean(avg_test_loss, axis=0)\n",
    "        gtsa_mean = np.mean(avg_test_accuracy, axis=0)\n",
    "\n",
    "        ptrl_mean = np.mean(avg_per_train_loss, axis=0)\n",
    "        ptra_mean = np.mean(avg_per_train_accuracy, axis=0)\n",
    "        ptsl_mean = np.mean(avg_per_test_loss, axis=0)\n",
    "        ptsa_mean = np.mean(avg_per_test_accuracy, axis=0)\n",
    "\n",
    "        gp_mean = np.mean(avg_precision, axis=0)\n",
    "        gr_mean = np.mean(avg_recall, axis=0)\n",
    "        gf1_mean = np.mean(avg_f1, axis=0)\n",
    "\n",
    "        pp_mean = np.mean(avg_per_precision, axis=0)\n",
    "        pr_mean = np.mean(avg_per_recall, axis=0)\n",
    "        pf1_mean = np.mean(avg_per_f1, axis=0)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        gtrl_std = np.std(avg_train_loss, axis=0)\n",
    "        gtra_std = np.std(avg_train_accuracy, axis=0)\n",
    "        gtsl_std = np.std(avg_test_loss, axis=0)\n",
    "        gtsa_std = np.std(avg_test_accuracy, axis=0)\n",
    "\n",
    "        ptrl_std = np.std(avg_per_train_loss, axis=0)\n",
    "        ptra_std = np.std(avg_per_train_accuracy, axis=0)\n",
    "        ptsl_std = np.std(avg_per_test_loss, axis=0)\n",
    "        ptsa_std = np.std(avg_per_test_accuracy, axis=0)\n",
    "\n",
    "        gp_std = np.std(avg_precision, axis=0)\n",
    "        gr_std = np.std(avg_recall, axis=0)\n",
    "        gf1_std = np.std(avg_f1, axis=0)\n",
    "\n",
    "        pp_std = np.std(avg_per_precision, axis=0)\n",
    "        pr_std = np.std(avg_per_recall, axis=0)\n",
    "        pf1_std = np.std(avg_per_f1, axis=0)\n",
    "        \n",
    "\n",
    "\n",
    "        gtrl_mean_std = np.column_stack((gtrl_mean, gtrl_std))\n",
    "        gtra_mean_std = np.column_stack((gtra_mean, gtra_std))\n",
    "        gtsl_mean_std = np.column_stack((gtsl_mean, gtsl_std))\n",
    "        gtsa_mean_std = np.column_stack((gtsa_mean, gtsa_std))\n",
    "\n",
    "        ptrl_mean_std = np.column_stack((ptrl_mean, ptrl_std))\n",
    "        ptra_mean_std = np.column_stack((ptra_mean, ptra_std))\n",
    "        ptsl_mean_std = np.column_stack((ptsl_mean, ptsl_std))\n",
    "        ptsa_mean_std = np.column_stack((ptsa_mean, ptsa_std))\n",
    "\n",
    "        gp_mean_std = np.column_stack((gp_mean, gp_std))\n",
    "        gr_mean_std = np.column_stack((gr_mean, gr_std))\n",
    "        gf1_mean_std = np.column_stack((gf1_mean, gf1_std))\n",
    "\n",
    "        pp_mean_std = np.column_stack((pp_mean, pp_std))\n",
    "        pr_mean_std = np.column_stack((pr_mean, pr_std))\n",
    "        pf1_mean_std = np.column_stack((pf1_mean, pf1_std))\n",
    "\n",
    "        training_loss_mean_std = gtrl_mean_std[gtrl_mean_std[:,0].argmin()]\n",
    "        training_acc_mean_std = gtra_mean_std[gtra_mean_std[:,0].argmax()]\n",
    "        val_loss_mean_std = gtsl_mean_std[gtsl_mean_std[:,0].argmin()]\n",
    "        val_acc_mean_std = gtsa_mean_std[gtsa_mean_std[:,0].argmax()]\n",
    "\n",
    "        per_training_loss_mean_std = ptrl_mean_std[gtrl_mean_std[:,0].argmin()]\n",
    "        per_training_acc_mean_std = ptra_mean_std[gtra_mean_std[:,0].argmax()]\n",
    "        per_val_loss_mean_std = ptsl_mean_std[gtsl_mean_std[:,0].argmin()]\n",
    "        per_val_acc_mean_std = ptsa_mean_std[gtsa_mean_std[:,0].argmax()]\n",
    "\n",
    "        precision_mean_std = gp_mean_std[gp_mean_std[:,0].argmax()]\n",
    "        recall_mean_std = gr_mean_std[gr_mean_std[:,0].argmax()]\n",
    "        f1_mean_std = gf1_mean_std[gf1_mean_std[:,0].argmax()]\n",
    "\n",
    "        per_precision_mean_std = pp_mean_std[pp_mean_std[:,0].argmax()]\n",
    "        per_recall_mean_std = pr_mean_std[pr_mean_std[:,0].argmax()]\n",
    "        per_f1_mean_std = pf1_mean_std[pf1_mean_std[:,0].argmax()]\n",
    "        \n",
    "        \n",
    "\n",
    "        with h5py.File(directory_name  + '{}.h5'.format(avg_file), 'w') as hf:\n",
    "            hf.create_dataset('avg_training_loss', data=gtrl_mean)\n",
    "            hf.create_dataset('avg_training_accuracy', data=gtra_mean)\n",
    "            hf.create_dataset('avg_test_loss', data=gtsl_mean)\n",
    "            hf.create_dataset('avg_test_accuracy', data=gtsa_mean)\n",
    "            \n",
    "            hf.create_dataset('avg_per_training_loss', data=ptrl_mean)\n",
    "            hf.create_dataset('avg_per_training_accuracy', data=ptra_mean)\n",
    "            hf.create_dataset('avg_per_test_loss', data=ptsl_mean)\n",
    "            hf.create_dataset('avg_per_test_accuracy', data=ptsa_mean)\n",
    "\n",
    "            hf.create_dataset('avg_precision', data=gp_mean)\n",
    "            hf.create_dataset('avg_recall', data=gr_mean)\n",
    "            hf.create_dataset('avg_f1', data=gf1_mean)\n",
    "\n",
    "            hf.create_dataset('avg_per_precision', data=pp_mean)\n",
    "            hf.create_dataset('avg_per_recall', data=pr_mean)\n",
    "            hf.create_dataset('avg_per_f1', data=pf1_mean)\n",
    "\n",
    "            hf.close\n",
    "\n",
    "\n",
    "\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\")\n",
    "        print(\"\")\n",
    "        print(f\"Global training loss (mean/std) : ({training_loss_mean_std[0]} / {training_loss_mean_std[1]})\")\n",
    "        print(f\"Global training accuracy (mean/std) : ({training_acc_mean_std[0]} / {training_acc_mean_std[1]})\")\n",
    "        print(f\"Global test loss (mean/std) : ({val_loss_mean_std[0]} / {val_loss_mean_std[1]})\")\n",
    "        print(f\"Global test accuracy (mean/std) : ({val_acc_mean_std[0]} / {val_acc_mean_std[1]})\")\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")\n",
    "        print(f\"Personalized training loss (mean/std) : ({per_training_loss_mean_std[0]} / {per_training_loss_mean_std[1]})\")\n",
    "        print(f\"Personalized training accuracy (mean/std) : ({per_training_acc_mean_std[0]} / {per_training_acc_mean_std[1]})\")\n",
    "        print(f\"Personalized test loss (mean/std) : ({per_val_loss_mean_std[0]} / {per_val_loss_mean_std[1]})\")\n",
    "        print(f\"Personalized test accuracy (mean/std) : ({per_val_acc_mean_std[0]} / {per_val_acc_mean_std[1]})\")  \n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")  \n",
    "        print(f\"Global Precision (mean/std) : ({precision_mean_std[0]} / {precision_mean_std[1]})\")\n",
    "        print(f\"Global Recall (mean/std) : ({recall_mean_std[0]} / {recall_mean_std[1]})\")\n",
    "        print(f\"Global F1Score (mean/std) : ({f1_mean_std[0]} / {f1_mean_std[1]})\")\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")  \n",
    "        print(f\"Personalized Precision (mean/std) : ({per_precision_mean_std[0]} / {per_precision_mean_std[1]})\")\n",
    "        print(f\"Personalized Recall (mean/std) : ({per_recall_mean_std[0]} / {per_recall_mean_std[1]})\")\n",
    "        print(f\"Personalized F1Score (mean/std) : ({per_f1_mean_std[0]} / {per_f1_mean_std[1]})\")\n",
    "        print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\") \n",
    "\n",
    "        file_name = 'performance_' + algorithm + '_' + str(target) + '.txt'\n",
    "        # Open a file to write\n",
    "        with open(directory_name + '/' + file_name, 'w') as file:\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\")\n",
    "            file.write(f\"algorithm : {algorithm} : target {target}\")\n",
    "            print_stats(\"Global training loss\", training_loss_mean_std, file)\n",
    "            print_stats(\"Global training accuracy\", training_acc_mean_std, file)\n",
    "            print_stats(\"Global test loss\", val_loss_mean_std, file)\n",
    "            print_stats(\"Global test accuracy\", val_acc_mean_std, file)\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\\n\")\n",
    "            print_stats(\"Personalized training loss\", per_training_loss_mean_std, file)\n",
    "            print_stats(\"Personalized training accuracy\", per_training_acc_mean_std, file)\n",
    "            print_stats(\"Personalized test loss\", per_val_loss_mean_std, file)\n",
    "            print_stats(\"Personalized test accuracy\", per_val_acc_mean_std, file)\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\\n\")\n",
    "            print_stats(\"Global Precision\", precision_mean_std, file)\n",
    "            print_stats(\"Global Recall\", recall_mean_std, file)\n",
    "            print_stats(\"Global F1Score\", f1_mean_std, file)\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\\n\")\n",
    "            print_stats(\"Personalized Precision\", per_precision_mean_std, file)\n",
    "            print_stats(\"Personalized Recall\", per_recall_mean_std, file)\n",
    "            print_stats(\"Personalized F1Score\", per_f1_mean_std, file)\n",
    "            file.write(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\\n\\n\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_exp_no_2_GR_50_BS_64.h5\n",
      "_exp_no_1_GR_50_BS_64.h5\n",
      "_exp_no_3_GR_50_BS_64.h5\n",
      "_exp_no_0_GR_50_BS_64.h5\n",
      "_exp_no_4_GR_50_BS_64.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/proj/sourasb-220503/FedMEM/results/ResNet50TL/FeSEM/10/40.0/h5/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m directory_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/proj/sourasb-220503/FedMEM/results/convergence/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43maverage_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFesem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFesem_10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 86\u001b[0m, in \u001b[0;36maverage_result\u001b[0;34m(path, directory_name, algorithm, avg_file, target)\u001b[0m\n\u001b[1;32m     82\u001b[0m gr_mean_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((gr_mean, gr_std))\n\u001b[1;32m     83\u001b[0m gf1_mean_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((gf1_mean, gf1_std))\n\u001b[0;32m---> 86\u001b[0m training_loss_mean_std \u001b[38;5;241m=\u001b[39m gtrl_mean_std[gtrl_mean_std[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmin()]\n\u001b[1;32m     87\u001b[0m training_acc_mean_std \u001b[38;5;241m=\u001b[39m gtra_mean_std[gtra_mean_std[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[1;32m     88\u001b[0m val_loss_mean_std \u001b[38;5;241m=\u001b[39m gtsl_mean_std[gtsl_mean_std[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmin()]\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "path = \"/proj/sourasb-220503/FedMEM/results/ResNet50TL/FeSEM/10/40.0/h5/\"\n",
    "directory_name = \"/proj/sourasb-220503/FedMEM/results/convergence/\"\n",
    "average_result(path, directory_name, 'Fesem', 'Fesem_10',10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalized_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
