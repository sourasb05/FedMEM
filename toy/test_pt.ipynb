{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 14, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0436, 0.0524, 0.0730,  ..., 0.0716, 0.1524, 0.0000],\n",
       "         [0.0597, 0.0426, 0.1641,  ..., 0.0309, 0.0000, 0.0000],\n",
       "         [0.1100, 0.0910, 0.0389,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.1121, 0.1374, 0.2056,  ..., 0.0000, 0.1092, 0.4703],\n",
       "         [0.1712, 0.1892, 0.2487,  ..., 0.1524, 0.2307, 0.2432],\n",
       "         [0.0916, 0.1942, 0.1454,  ..., 0.8572, 0.4561, 0.4674]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1122, 0.4022],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0738, 0.1464],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0014, 0.1057,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0513,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.1898,  ..., 0.0421, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3187, 0.1919, 0.0148,  ..., 0.5042, 0.2198, 0.1559],\n",
       "         [0.2731, 0.4171, 0.5633,  ..., 0.1583, 0.0000, 0.0068],\n",
       "         [0.3816, 0.3396, 0.3187,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.1484,  ..., 0.5339, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.2510,  ..., 0.1979, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.1333,  ..., 0.0479, 0.1430, 0.0000],\n",
       "         [0.0193, 0.0000, 0.0000,  ..., 0.3311, 0.4774, 0.0000],\n",
       "         [0.0140, 0.0000, 0.2728,  ..., 0.4573, 0.3909, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.2379, 0.1299,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1134, 0.0327],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0778, 0.1064],\n",
       "         [0.0000, 0.0204, 0.0387,  ..., 0.0000, 0.3401, 0.1010],\n",
       "         [0.0622, 0.1990, 0.0687,  ..., 0.0000, 0.0062, 0.0304]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0400,  ..., 0.1820, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1517, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Replace 'path_to_your_file.pt' with the actual path to your .pt file\n",
    "file_path = '/proj/sourasb-220503/FedMEM/dataset/r3_mem_ResNet50FC_features/NEUR16_scan1_img01_day22_S.pt'\n",
    "\n",
    "# Load the data\n",
    "data = torch.load(file_path)\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1768, -0.1486, -0.2856,  ...,  1.2214,  1.2214,  1.1872],\n",
       "         [ 1.1358,  0.4508, -0.1828,  ...,  1.2043,  1.2043,  1.1529],\n",
       "         [ 1.9920,  1.2043,  0.0227,  ...,  1.1700,  1.1700,  1.1358],\n",
       "         ...,\n",
       "         [-0.9877, -0.9534, -0.9363,  ..., -0.7650, -0.7822, -0.7650],\n",
       "         [-0.9020, -0.9192, -0.9192,  ..., -0.8164, -0.7993, -0.7822],\n",
       "         [-0.9192, -0.9534, -0.9363,  ..., -0.7308, -0.7479, -0.9020]],\n",
       "\n",
       "        [[ 0.2402, -0.0924, -0.2325,  ...,  1.5007,  1.5007,  1.5357],\n",
       "         [ 1.2206,  0.5203, -0.1275,  ...,  1.4832,  1.4832,  1.5182],\n",
       "         [ 2.1134,  1.2906,  0.0826,  ...,  1.4482,  1.4482,  1.4657],\n",
       "         ...,\n",
       "         [-0.8627, -0.8978, -0.8803,  ..., -0.7752, -0.7752, -0.7752],\n",
       "         [-0.7927, -0.8102, -0.8102,  ..., -0.7752, -0.7577, -0.7577],\n",
       "         [-0.8452, -0.7927, -0.7752,  ..., -0.8102, -0.8277, -0.7402]],\n",
       "\n",
       "        [[ 0.4439,  0.1128, -0.0267,  ...,  2.6226,  2.6226,  2.6226],\n",
       "         [ 1.4200,  0.7228,  0.0779,  ...,  2.6226,  2.6226,  2.6226],\n",
       "         [ 2.2914,  1.4897,  0.2871,  ...,  2.6226,  2.6226,  2.6051],\n",
       "         ...,\n",
       "         [-0.5844, -0.5844, -0.5844,  ..., -0.7413, -0.7413, -0.6715],\n",
       "         [-0.5670, -0.5670, -0.5844,  ..., -0.7238, -0.7064, -0.6890],\n",
       "         [-0.6541, -0.6367, -0.6367,  ..., -0.6715, -0.7064, -0.8458]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/proj/sourasb-220503/FedMEM/dataset/r3_refined/NEUR16_scan1_img01_day22_S.jpg'\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 256x256 pixels\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize pixel values (example for RGB)\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply the transformations to the image\n",
    "tensor_image = transform(image)\n",
    "print(tensor_image.shape)\n",
    "tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 224, 224])\n",
      "torch.Size([1024, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand([1024, 14, 14])  # Simulated data; replace with your actual tensor\n",
    "labels = torch.randint(0, 10, (1024,))  # Example target labels for a 10-class problem\n",
    "data_upsampled = F.interpolate(data.unsqueeze(1), size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "\n",
    "print(data_upsampled.shape)\n",
    "\n",
    "\n",
    "data_upsampled = data_upsampled.repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "print(data_upsampled.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_souba/anaconda3/envs/personalized_fl/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/x_souba/anaconda3/envs/personalized_fl/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/x_souba/anaconda3/envs/personalized_fl/lib/python3.11/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.217625856399536\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/personalized_fl/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/personalized_fl/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(data_upsampled, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load a pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer to match the number of classes in your dataset (e.g., 10)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # Assuming 10 classes\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalized_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
